---
title: "Section 2 - Requirements for first-party developers"
permalink: /statutes/section2/
excerpt: "Section 2 of the model statute"
last_modified_at: 2024-11-06
toc: true
---
### Section 2.1 - Assessment requirements, general {#section-2.1}
* (a) Any first-party developer of an AI system shall be required to do the following:
  * (1)  Internally document the training and evaluation process, specifically:
    * (A) The provenance of any training and evaluation data,
    * (B) The person(s) responsible for maintenance of the system, and
    * (C) Any risk assessments that have been completed for the system.
  * (2) Conduct rigorous testing of the AI system, specifically:
    * (A) Testing for disparaties in performance for marginalized communities,
    * (B) Evaluating the system's performance for inputs that do not match the properties of any data on which the system was trained or evaluated ("out of distribution" data), and
    * (C) ...
* (b) The Commission on AI Technology, as defined in [Section 4]({{ "/statutes/section4/" | relative_url }}), shall have the right to examine a first-party developer's compliance with the requirements of Section 2.1(a) as they see fit. If the developer is found to be non-compliant, penalties as defined in [Section 6]({{ "/statutes/section6/" | relative_url }}) can apply. 
#### Committee notes - Section 2.1
* (1)  These requirements are purposely broad because reasons. 
#### Applicable resources - Section 2.1
* Tools: 
* Principles: 
* Scenarios: 

### 2.2 - Assessment requirements, sensitive applications {#section-2.2}

In addition to the requirements of Section 2.1, if an AI system is used for the purpose of a sensitive application, as defined in Section 1(d), the following requirements apply:
* (a) The first-party developer must quarterly report to the Commission on:
  * (1) compliance with the requirements of Section 2.1, and
  * (2) Any consumer reports of harms caused by the system.
* (b) The first-party developer must annually report to the Commission on:
  * (1) any plans to update the system in a way that can substantially affect its performance.
#### Applicable resources - Section 2.2
* Tools: 
* Principles: 
* Scenarios: 

### 2.3 - Forbidden applications {#section-2.3}
Development of AI systems for the following purposes are prohibited:
* (a) Determining a person's race, sex, gender, or sexual orientation,
* (b) Controlling a deadly weapon autonomously,
* (c) Impersonation of an individual, and
* (d) Purposeful generation of disinformation or misinformation.
#### Committee notes - Section 2.3
* (1) The rationale for banning these applications is two-fold. Some, such as in (a), are scientifically unsound applications of AI technology. Others, like (b)-(d), have the potential to inflict large-scale bodily or psychological harms on an individual.

### Section 2.4 - Claims about or by AI systems {#section-2.4}
* (a) False or misleading claims regarding the capabilities of an AI system shall be considered a deceptive act or practice under applicable federal or state consumer protection law. 
* (b) If an AI system itself engages in a deceptive act or practice, any and all first-party developers of the system shall be held jointly and severally liable for any harms that result from the practice. 
#### Committee notes - Section 2.4
* (1) An example of 2.4(b) would be an AI customer support chatbot giving a customer incorrect information about their ability to obtain a refund. The parent company is to be held liable in the same way they would if a human support agent gave the incorrect information.
#### Applicable resources - Section 2.4
* Tools: 
* Principles: 
* Scenarios: 
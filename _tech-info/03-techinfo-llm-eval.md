---
title: "Evaluating language models for accuracy and bias"
permalink: /tech-info/llm-eval/
excerpt: "Tools for model documentation"
last_modified_at: 2024-11-06
toc: false
---

Some tools for evaluating large language models include:

* [OpenAI evals](https://github.com/openai/evals): OpenAI's LLM evaluation tool, with a benchmark repository
* [Evidently](https://github.com/evidentlyai/evidently/): an ML and LLM observability tool which can be used on general ML tasks, including LLMs


### Applicable statutes

* [Section 2.3]({{ "statutes/section2.3/" | relative_url }})